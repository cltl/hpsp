## Wed 27 Jun 2018

Some simple statistics (see `notebook/stats-ontonotes-triples.ipynb`) revealed 
that OntoNotes has poor coverage w.r.t. Wang et al.'s dataset. I would be
better off using an unannotated dataset. 

## Sun 6 Jan 2019

Shape up the triple-generation notebook.

User Drake to record experimental process.

## Sun 3 Feb 2019

Managed to train a few models on ukWaC data. I didn't find any difference
between tanh and normal activation function, both in terms of summary 
performance and an analysis of accurary versus frequency in the training
corpus.

<s>One hypothesis for this lack of difference is that the constraints that
the training problem put on the model is so loose that either model can
fit equally well. 
So one way to fix this is to add more constraints to the problem, for
example to "retrofit" WordNet hierarchy into the vector space.</s>
This hypothesis is likely wrong because the models are underfitting
-- accuracy measured during training is never higher than 70%.

I still think if I dig deeper into the receptive field of hidden neurons
I might find some difference. But the first problem is why are the 
networks underfitting? I remember trying to increase the number of
hidden layer but it didn't overfit either. 
It doesn't get the reported 89% even in training.
I'll contact the author about this first.